
Digit Recognizer- Handwritten Digit Classification Kaggle Competition


This document outlines the performance of 3 different machine learning algorithms--random forests, K-nearest neighbors and stochastic gradient boosting--run on custom features extracted from the MNIST data set, a collection of 28x28 pixel handwritten digits. This is a well known data set that has been studied and crunched for years; there are neural networks and other techniques out there achieving 99% test set accuracy. This write up details a first brush attempt with no foreknowledge of the known best features or methods to tackle this problem.


Begin by loading in some useful R libraries:
```{r}
library(caret)
library(randomForest)
library(gbm)
library(kknn)
library(ggplot2)
```


Next I read the data and separate the labels from the pixel data
```{r}
train = read.csv("train.csv")
test= read.csv("test.csv")

labels= as.factor(train[ ,1])
train = train[ ,2:785]
```


I Combine the training and and test sets temporarily when creating new features
```{r}
train = rbind(train,test)
```


Next I Create some potentially useful meta features for training. Each training case is a vector of 784 pixel intensity values, from 0-256.

```{r}
#Total pixel intensity
total_inensity = rowSums(train)
#Number of pixels with non-zero intensity
non_zero = rowSums(train!=0)
#Average pixel intensity
average_inensity = total_inensity/non_zero
#Number of low inensity pixels
between_0_100 = rowSums(train>0 & train<100)
#Proportion of low inensity pixels
prop_between_0_100 = between_0_100/non_zero
```

I put the new features in a new training data frame
```{r}
new_train=data.frame("total_inensity"=total_inensity,"non_zero"=non_zero,"average_inensity"=average_inensity,"between_0_100"=between_0_100,"prop_between_0_100"=prop_between_0_100)
```


Over the next 40 lines, I create new features that attempt to capture some of the structure of the images. I make 28 features for the proportion of an image's total intensity contained in each 1 pixel row do the same for each 1 pixel column. I create 16 features based on intensity in each large patch when splitting the image into a 4x4 grid and 49 features based on intensity of each small section when splitting an image into a 7x7 grid. This reduces the total number of features from 784 to 126. My hope is that this will reduce the feature space without throwing out too much useful information.

```{r}
vertical_seq = seq(1,784,by=28)

#Features for proportion of total pixel intensity in horizontal strips
for (x in 1:28){
  row_intensity = rowSums(train[,(1+(28*(x-1))):(28+(28*(x-1)))])
  new_train[paste("h",x,sep="")] = row_intensity/total_inensity
}

#Features for proportion of total pixel intensity in vertical strips
for (x in 1:28){
  new_train[paste("v",x,sep="")] = rowSums(train[,vertical_seq+(x-1)])/total_inensity
}

#Features for proportion of total pixel intensity in large patches (splits the image into a 4x4 grid and finds the proportion of total pixel inensity in each section)
for (x in 1:4){
  for (y in 1:4){
    line = 1:7 + ((x-1)*7) + ((y-1)*196)
    for (z in 1:6){
      line=c(line,line[1:7]+(28*z))
    }
    new_train[paste("section",x,y,sep="")] = rowSums(train[line])/total_inensity
  }
}

#Features for proportion of total pixel intensity in small patches (splits the image into a 7x7 grid and finds the proportion of total pixel inensity in each section)
for (x in 1:7){
  for (y in 1:7){
    line = 1:4 + ((x-1)*4) + ((y-1)*112)
    for (z in 1:3){
      line=c(line,line[1:4]+(28*z))
    }
    new_train[paste("smallsection",x,y,sep="")] = rowSums(train[line])/total_inensity
  }
}
```


Now I separate the testing data from the training data and create a smaller partial training set and validation set.

```{r}
test = new_train[42001:70000, ]
new_train= new_train[1:42000,]

#Training and validation sets
part_train = new_train[1:30000,]
valid = new_train[30001:42000,]
```


I begin by training a random forest model.

```{r}
#Random forest model
set.seed(12)
rf_mod1= randomForest(labels[1:30000]~., data=part_train, nodesize=1, ntree=250, mtry=5)

rf_pred = predict(rf_mod1,newdata=valid)

confusionMatrix(rf_pred,labels[30001:42000])
```

The confusion matrix shows an overall validation accuracy of ~ 0.96


Next, a KNN Model
```{r}
knnpred = kknn(labels[1:30000]~., train=part_train, test=valid, k=3)
kpred = fitted.values(knnpred)

confusionMatrix(fitted.values(knnpred),labels[30001:42000])
```

KNN comes in with a validation Accuracy of 0.9385



Finally I train a stochastic gradient boosting model using the caret package.
```{r}
tunecontrol = trainControl(method = "repeatedcv",
                           number = 2,
                           repeats = 1
)

tgrid = expand.grid(n.trees = c(100),interaction.depth=c(7) ,shrinkage=c(0.107) )

gbm_mod = train(labels[1:30000]~., data=part_train, method= 'gbm', trControl=tunecontrol, tuneGrid=tgrid)

pred_gbm = predict(gbm_mod, newdata=valid)

confusionMatrix(pred_gbm,labels[30001:42000])
```
GBM achieves a Validation Accuracy of 0.95916


As a final step, I combine the 3 classifiers into an ensemble prediction. If 2 or 3 classifiers make the same prediction, I choose that prediction, otherwise defer I to the random forest model.
```{r}
comb_pred = as.factor(ifelse(pred_gbm==kpred,kpred,rf_pred))
levels(comb_pred) = 0:9

confusionMatrix(comb_pred,labels[30001:42000])
```
The ensemble increases validation accuracy to 0.967


Using the full training data and full test data I'd expect similar performance. Rerunning the models on the full training set and submitting to Kaggle gives a test set accuracy of 0.96443. While this accuracy is much lower than models using the best feature extraction methods and neural networks, it shows that respectable accuracy is possible with limited domain knowledge, simple models and limited computing power.


As a final wrap up, I thought it would be interesting to plot some of the images that the final ensemble classified incorrectly. 


```{r}
plotter = function(img_num){
  image(t(matrix(as.matrix(train[img_num,]),nrow=28,byrow=TRUE)[28:1,]), axes = FALSE, col = grey(seq(1, 0, length = 256)))
}
  
first_10_missclassified = head(which(comb_pred!=labels[30001:42000]), 10)
for (error in first_10_missclassified){
  print(paste("Estimate: ",comb_pred[error]))
  print(paste("Actual: ",labels[30001:42000][error]))
  plotter(30000+error)
}
```

And I thought my handwriting was bad!
