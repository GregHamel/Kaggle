Poker Rule Induction Kaggle competition

#https://www.kaggle.com/c/poker-rule-induction

This competition tasks entrants with classifying poker hands based on a sample of 25010 labeled hands. The data is separated into 10 features, indicating the suit and rank of each card. Cards are not given in any particular order. The difficulty is learning rules without making any assumptions about the rules and using domain knowledge to guide learning. Of course it would be easy to sort cards, calculate combinations or hard code poker rules to achieve good results, but the challenge states that you are to act as though you are an alien that doesn't know anything about poker or card games. The methods used are supposed to be general enough to apply to completely different card games with different rules. Several entrants are scoring 99% or above in accuracy, which suggests that people are injecting domain knowledge into solutions. My aim is to run a few basic machine learning algorithms on the raw data without making any assumptions and beat the competition given random forest benchmark accuracy of 0.62408.  

I Start by loading some machine learning libraries.
```{r}
library(caret)
library(randomForest)
library(gbm)
options( java.parameters = "-Xmx3g" )
library(extraTrees)
```


Next I read in the training and test data
```{r}
train = read.csv("train.csv")
test = read.csv("test.csv")
#Get rid of ID column
test = test[,2:11]

#Separate labels from training set
labels = as.factor(train$hand)
train = train[,1:10]

#Split training set into partial training set and validation set
part_train = train[1:18000,]
valid = train[-1:-18000,]

labels_part = labels[1:18000]
valid_labels = labels[-1:-18000]
```

First I run a random forest model
```{r}
set.seed(12)
tree = randomForest(labels_part~., data=part_train, nodesize=1, ntree=500, mtry=4)

tree_pred = predict(tree, newdata=valid, type="class")

confusionMatrix(tree_pred,valid_labels)
```
Accuracy of 0.6161. Slightly lower than the benchmark.


Next I try an extra trees classifier.
```{r}
set.seed(12)
xtrees = extraTrees(y=labels_part, x=part_train, nodesize=1, ntree=500, mtry=4, numRandomCuts = 3)

xtrees_pred = predict(xtrees, newdata=valid)

confusionMatrix(xtrees_pred,valid_labels)
```
Accuracy of 0.6036. Again, a bit lower than the benchmark. Reaching the benchmark is likely a matter of parameter tuning.


Finally, I try a gbm model.
```{r}
set.seed(12)

tunecontrol = trainControl(method = "none")

tgrid = expand.grid(n.trees = c(100), interaction.depth=c(15) ,shrinkage=c(0.107) )

gbm_mod = train(labels_part~., data=part_train, method="gbm", trControl=tunecontrol, tuneGrid=tgrid)

pred_gbm = predict(gbm_mod, newdata=valid)
confusionMatrix(pred_gbm ,valid_labels)
```

Accuracy of 0.6498, beating the benchmark. Increasing n.trees to 2000 gives a validation accuracy of 0.7692.

Rerunning the model on the full training set with the same parameters and submitting the result to Kaggle gives a leaderboard score of 0.81873. Since the extra training data appears to have significantly improved accuracy, generating new training examples or letting the algorithm run longer(more than 2000 trees) may improve accuracy. Since I'm currently at my computer's 8GB RAM limit so I'm content to let this competition rest with a single submission.
